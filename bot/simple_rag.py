"""–ü—Ä–æ—Å—Ç–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ LangChain (–∫–∞–∫ –≤ naive-rag.ipynb)"""

import logging
import tempfile
import os
from typing import Dict, Any, List, Optional
from pathlib import Path

try:
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_core.vectorstores import InMemoryVectorStore
    from langchain_openai import OpenAIEmbeddings, ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.runnables import RunnablePassthrough
    from langchain_core.messages import HumanMessage, AIMessage
except ImportError as e:
    logging.warning(f"LangChain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
    PyPDFLoader = None
    RecursiveCharacterTextSplitter = None
    InMemoryVectorStore = None
    OpenAIEmbeddings = None
    ChatOpenAI = None
    ChatPromptTemplate = None
    StrOutputParser = None
    RunnablePassthrough = None
    HumanMessage = None
    AIMessage = None
    MessagesPlaceholder = None

logger = logging.getLogger(__name__)


class SimpleRAG:
    """–ü—Ä–æ—Å—Ç–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ LangChain (–∫–∞–∫ –≤ notebook)"""
    
    def __init__(self):
        if not all([PyPDFLoader, RecursiveCharacterTextSplitter, InMemoryVectorStore]):
            raise ImportError("LangChain –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
        
        self.vector_store = None
        self.retriever = None
        self.llm = None
        self.llm_query_transform = None
        self.rag_chain = None
        self.rag_conversation_chain = None
        self.rag_query_transform_chain = None
        self._initialize_components()
    
    def _initialize_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ RAG"""
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π LLM (–∏—Å–ø–æ–ª—å–∑—É–µ–º OpenRouter –≤–º–µ—Å—Ç–æ OpenAI)
            self.llm = ChatOpenAI(
                model="meta-llama/llama-3.3-70b-instruct:free", 
                temperature=0.9,
                openai_api_base="https://openrouter.ai/api/v1",
                openai_api_key=os.getenv("OPENROUTER_API_KEY")
            )
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LLM –¥–ª—è Query Transformation (–∫–∞–∫ –≤ notebook)
            self.llm_query_transform = ChatOpenAI(
                model="meta-llama/llama-3.3-70b-instruct:free",
                temperature=0.4,
                openai_api_base="https://openrouter.ai/api/v1",
                openai_api_key=os.getenv("OPENROUTER_API_KEY")
            )
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–∫–∞–∫ –≤ notebook)
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAI API –¥–ª—è embeddings (–∫–∞–∫ –≤ notebook)")
            self.embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
            
            logger.info("RAG –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —Å OpenRouter")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
            raise
    
    def process_pdf(self, file_path: str) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–∞ (–∫–∞–∫ –≤ notebook)
        
        Args:
            file_path: –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        try:
            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é PDF: {file_path}")
            
            # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∫–∞–∫ –≤ notebook)
            loader = PyPDFLoader(
                file_path=file_path,
                mode="page",
                extraction_mode="plain"
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            pages = []
            for page in loader.load():
                pages.append(page)
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(pages)} —Å—Ç—Ä–∞–Ω–∏—Ü")
            
            # 2. –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
            all_splits = self._smart_chunk_split(pages, chunk_size=400, overlap=100)
            
            logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(all_splits)} —á–∞–Ω–∫–æ–≤")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏
            self._analyze_chunks_quality(pages, all_splits)
            
            # 3. –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (–∫–∞–∫ –≤ notebook)
            logger.info("–°–æ–∑–¥–∞—é –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ...")
            
            # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (–∫–∞–∫ –≤ notebook)
            self.vector_store = InMemoryVectorStore.from_documents(
                all_splits,
                embedding=self.embeddings
            )
            logger.info(f"–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ–∑–¥–∞–Ω–æ —Å {len(all_splits)} —á–∞–Ω–∫–∞–º–∏")
            for i, chunk in enumerate(all_splits):
                logger.info(f"–ß–∞–Ω–∫ {i+1} –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏: {chunk.page_content[:150]}...")
            
            # 4. –°–æ–∑–¥–∞–Ω–∏–µ retriever (–∫–∞–∫ –≤ notebook)
            self.retriever = self.vector_store.as_retriever(
                search_kwargs={'k': 3}
            )
            
            # 5. –°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö RAG —Ü–µ–ø–æ—á–µ–∫ (–∫–∞–∫ –≤ notebook)
            self._create_rag_chains()
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_preview = self._create_content_preview(pages)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = self._extract_metadata(file_path, pages)
            
            return {
                'success': True,
                'pages': len(pages),
                'chunks_count': len(all_splits),
                'content_preview': content_preview,
                'metadata': metadata
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF: {e}")
            return {
                'success': False,
                'error': f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}'
            }
    
    def _create_rag_chains(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö RAG —Ü–µ–ø–æ—á–µ–∫ (–∫–∞–∫ –≤ notebook)"""
        try:
            # 1. –ë–∞–∑–æ–≤–∞—è RAG —Ü–µ–ø–æ—á–∫–∞ (–∫–∞–∫ –≤ notebook)
            self._create_basic_rag_chain()
            
            # 2. Conversational RAG —Ü–µ–ø–æ—á–∫–∞ (–∫–∞–∫ –≤ notebook)
            self._create_conversational_rag_chain()
            
            # 3. RAG —Å Query Transformation (–∫–∞–∫ –≤ notebook)
            self._create_query_transform_rag_chain()
            
            logger.info("–í—Å–µ RAG —Ü–µ–ø–æ—á–∫–∏ —Å–æ–∑–¥–∞–Ω—ã")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è RAG —Ü–µ–ø–æ—á–µ–∫: {e}")
            raise
    
    def format_chunks(self, chunks):
        """–û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É (–∫–∞–∫ –≤ notebook)"""
        return "\n\n".join(chunk.page_content for chunk in chunks)
    
    def _create_basic_rag_chain(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π RAG —Ü–µ–ø–æ—á–∫–∏ (–∫–∞–∫ –≤ notebook)"""
        try:
            # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–∫–∞–∫ –≤ notebook)
            SYSTEM_TEMPLATE = """
You are an assistant for question-answering tasks.
Do not use Chinese characters in respond.
Use the following pieces of retrieved context to answer the user question.
If you don't know the answer, just say '–Ø –Ω–µ –Ω–∞—à–µ–ª –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å!'.
Use three sentences maximum and keep the answer concise.

Context:
{context}
"""
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç —à–∞–±–ª–æ–Ω (–∫–∞–∫ –≤ notebook)
            question_answering_prompt = ChatPromptTemplate([
                ("system", SYSTEM_TEMPLATE),
                ("human", "{question}"),
            ])
            
            # –°–æ–∑–¥–∞–µ–º RAG —Ü–µ–ø–æ—á–∫—É (–∫–∞–∫ –≤ notebook)
            self.rag_chain = (
                {"context": self.retriever | self.format_chunks, "question": RunnablePassthrough()}
                | question_answering_prompt
                | self.llm
                | StrOutputParser()
            )
            
            logger.info("–ë–∞–∑–æ–≤–∞—è RAG —Ü–µ–ø–æ—á–∫–∞ —Å–æ–∑–¥–∞–Ω–∞")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑–æ–≤–æ–π RAG —Ü–µ–ø–æ—á–∫–∏: {e}")
            raise
    
    def _create_conversational_rag_chain(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ conversational RAG —Ü–µ–ø–æ—á–∫–∏ (–∫–∞–∫ –≤ notebook)"""
        try:
            # Conversational —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–∫–∞–∫ –≤ notebook)
            CONVERSATION_SYSTEM_TEMPLATE = """
You are an assistant for question-answering tasks. Do not use Chinese characters in respond. Answer the user's questions based on the conversation history and below context retrieved for the last question. Answer '–Ø –Ω–µ –Ω–∞—à–µ–ª –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å!' if you don't find any information in the context. Use three sentences maximum and keep the answer concise.

Context retrieved for the last question:

{context}
"""
            
            # –°–æ–∑–¥–∞–µ–º conversational –ø—Ä–æ–º–ø—Ç (–∫–∞–∫ –≤ notebook)
            conversational_answering_prompt = ChatPromptTemplate([
                ("system", CONVERSATION_SYSTEM_TEMPLATE),
                ("placeholder", "{messages}")
            ])
            
            # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è (–∫–∞–∫ –≤ notebook)
            def get_last_message_for_retriever_input(params: Dict):
                return params["messages"][-1].content
            
            # –°–æ–∑–¥–∞–µ–º conversational RAG —Ü–µ–ø–æ—á–∫—É (–∫–∞–∫ –≤ notebook)
            self.rag_conversation_chain = (
                RunnablePassthrough.assign(
                    context=get_last_message_for_retriever_input | self.retriever | self.format_chunks
                )
                | conversational_answering_prompt
                | self.llm
                | StrOutputParser()
            )
            
            logger.info("Conversational RAG —Ü–µ–ø–æ—á–∫–∞ —Å–æ–∑–¥–∞–Ω–∞")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è conversational RAG —Ü–µ–ø–æ—á–∫–∏: {e}")
            raise
    
    def _create_query_transform_rag_chain(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ RAG —Ü–µ–ø–æ—á–∫–∏ —Å Query Transformation (–∫–∞–∫ –≤ notebook)"""
        try:
            # –ü—Ä–æ–º–ø—Ç –¥–ª—è Query Transformation (–∫–∞–∫ –≤ notebook)
            retrieval_query_transform_prompt = ChatPromptTemplate.from_messages([
                MessagesPlaceholder(variable_name="messages"),
                (
                    "user",
                    "Transform last user message to a search query in Russian language according to the whole conversation history above to further retrieve the information relevant to the conversation. For general questions like 'what is this about?' or 'what is the article about?', search for: article topic, main theme, summary, abstract, main concepts, document content. For specific questions, search for exact information. Try to thoroughly analyze all messages to generate the most relevant query. The longer result better than short. Only respond with the query, nothing else.",
                ),
            ])
            
            # –°–æ–∑–¥–∞–µ–º —Ü–µ–ø–æ—á–∫—É Query Transformation (–∫–∞–∫ –≤ notebook)
            retrieval_query_transformation_chain = (
                retrieval_query_transform_prompt 
                | self.llm_query_transform 
                | StrOutputParser()
            )
            
            # Conversational –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤
            CONVERSATION_SYSTEM_TEMPLATE = """
You are an assistant for question-answering tasks. Do not use Chinese characters in respond. Answer the user's questions based on the conversation history and below context retrieved for the last question. Answer '–Ø –Ω–µ –Ω–∞—à–µ–ª –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å!' if you don't find any information in the context. Use three sentences maximum and keep the answer concise.

Context retrieved for the last question:

{context}
"""
            
            conversational_answering_prompt = ChatPromptTemplate([
                ("system", CONVERSATION_SYSTEM_TEMPLATE),
                ("placeholder", "{messages}")
            ])
            
            # –°–æ–∑–¥–∞–µ–º RAG —Ü–µ–ø–æ—á–∫—É —Å Query Transformation (–∫–∞–∫ –≤ notebook)
            self.rag_query_transform_chain = (
                RunnablePassthrough.assign(
                    context=retrieval_query_transformation_chain | self.retriever | self.format_chunks
                )
                | conversational_answering_prompt
                | self.llm
                | StrOutputParser()
            )
            
            logger.info("RAG —Ü–µ–ø–æ—á–∫–∞ —Å Query Transformation —Å–æ–∑–¥–∞–Ω–∞")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è RAG —Ü–µ–ø–æ—á–∫–∏ —Å Query Transformation: {e}")
            raise
    
    def _analyze_chunks_quality(self, pages: List, chunks: List) -> None:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        try:
            if not pages or not chunks:
                logger.warning("–ù–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü –∏–ª–∏ —á–∞–Ω–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                return
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
            full_text = ""
            for page in pages:
                full_text += page.page_content + "\n"
            
            logger.info("=" * 80)
            logger.info("–ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –†–ê–ó–ë–ò–ï–ù–ò–Ø –ù–ê –ß–ê–ù–ö–ò")
            logger.info("=" * 80)
            
            # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_text_length = len(full_text)
            total_chunks = len(chunks)
            total_chunk_length = sum(len(chunk.page_content) for chunk in chunks)
            
            logger.info(f"üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            logger.info(f"   ‚Ä¢ –î–ª–∏–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {total_text_length:,} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {total_chunks}")
            logger.info(f"   ‚Ä¢ –û–±—â–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–æ–≤: {total_chunk_length:,} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"   ‚Ä¢ –ü–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞: {(total_chunk_length/total_text_length)*100:.1f}%")
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤
            chunk_sizes = [len(chunk.page_content) for chunk in chunks]
            avg_size = sum(chunk_sizes) / len(chunk_sizes)
            min_size = min(chunk_sizes)
            max_size = max(chunk_sizes)
            
            logger.info(f"üìè –†–ê–ó–ú–ï–†–´ –ß–ê–ù–ö–û–í:")
            logger.info(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"   ‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {min_size} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"   ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max_size} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
            logger.info(f"üìù –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ß–ê–ù–ö–û–í:")
            for i, chunk in enumerate(chunks):
                chunk_text = chunk.page_content
                chunk_length = len(chunk_text)
                
                # –ù–∞—Ö–æ–¥–∏–º –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü —á–∞–Ω–∫–∞ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
                start_pos = full_text.find(chunk_text[:50])  # –ò—â–µ–º –ø–æ –ø–µ—Ä–≤—ã–º 50 —Å–∏–º–≤–æ–ª–∞–º
                end_pos = start_pos + chunk_length if start_pos != -1 else -1
                
                logger.info(f"   –ß–∞–Ω–∫ {i+1:2d}: {chunk_length:3d} —Å–∏–º–≤–æ–ª–æ–≤ | –ü–æ–∑–∏—Ü–∏—è: {start_pos:4d}-{end_pos:4d}")
                logger.info(f"              –ù–∞—á–∞–ª–æ: {chunk_text[:60]}...")
                logger.info(f"              –ö–æ–Ω–µ—Ü:   ...{chunk_text[-40:]}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ–±—Ä—ã–≤–∞–µ—Ç—Å—è –ª–∏ —á–∞–Ω–∫ –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                if chunk_text and chunk_text[-1] not in '.!?':
                    logger.warning(f"              ‚ö†Ô∏è  –ß–∞–Ω–∫ {i+1} –æ–±—Ä—ã–≤–∞–µ—Ç—Å—è –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è!")
                else:
                    logger.info(f"              ‚úÖ –ß–∞–Ω–∫ {i+1} –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ
            logger.info(f"üîç –ü–†–û–í–ï–†–ö–ê –ü–û–ö–†–´–¢–ò–Ø:")
            covered_positions = set()
            for chunk in chunks:
                chunk_text = chunk.page_content
                start_pos = full_text.find(chunk_text[:50])
                if start_pos != -1:
                    for pos in range(start_pos, start_pos + len(chunk_text)):
                        covered_positions.add(pos)
            
            total_positions = len(full_text)
            covered_count = len(covered_positions)
            coverage_percent = (covered_count / total_positions) * 100
            
            logger.info(f"   ‚Ä¢ –ü–æ–∫—Ä—ã—Ç–æ –ø–æ–∑–∏—Ü–∏–π: {covered_count:,} –∏–∑ {total_positions:,}")
            logger.info(f"   ‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è: {coverage_percent:.1f}%")
            
            if coverage_percent < 95:
                logger.warning(f"   ‚ö†Ô∏è  –ù–∏–∑–∫–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞! –í–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏.")
            
            # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è
            logger.info(f"üìà –û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê:")
            broken_chunks = sum(1 for chunk in chunks if chunk.page_content and chunk.page_content[-1] not in '.!?')
            quality_score = ((len(chunks) - broken_chunks) / len(chunks)) * 100 if chunks else 0
            
            logger.info(f"   ‚Ä¢ –ß–∞–Ω–∫–æ–≤ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º –æ–∫–æ–Ω—á–∞–Ω–∏–µ–º: {len(chunks) - broken_chunks}/{len(chunks)}")
            logger.info(f"   ‚Ä¢ –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {quality_score:.1f}%")
            
            if quality_score >= 80:
                logger.info(f"   ‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è: –û–¢–õ–ò–ß–ù–û")
            elif quality_score >= 60:
                logger.info(f"   ‚ö†Ô∏è  –ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è: –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û")
            else:
                logger.warning(f"   ‚ùå –ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è: –ü–õ–û–•–û - –Ω—É–∂–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")
            
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤: {e}")
    
    def _smart_chunk_split(self, pages: List, chunk_size: int = 400, overlap: int = 100) -> List:
        """–£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º –≥—Ä–∞–Ω–∏—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        try:
            from langchain_core.documents import Document
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
            full_text = ""
            for page in pages:
                full_text += page.page_content + "\n"
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            import re
            sentences = re.split(r'(?<=[.!?])\s+', full_text)
            
            chunks = []
            current_chunk = ""
            current_size = 0
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø—Ä–µ–≤—ã—Å–∏—Ç —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
                if current_size + len(sentence) > chunk_size and current_chunk:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                    chunks.append(Document(
                        page_content=current_chunk.strip(),
                        metadata={"source": "smart_split"}
                    ))
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
                    overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk
                    current_chunk = overlap_text + " " + sentence
                    current_size = len(current_chunk)
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∫ —Ç–µ–∫—É—â–µ–º—É —á–∞–Ω–∫—É
                    if current_chunk:
                        current_chunk += " " + sentence
                    else:
                        current_chunk = sentence
                    current_size = len(current_chunk)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            if current_chunk.strip():
                chunks.append(Document(
                    page_content=current_chunk.strip(),
                    metadata={"source": "smart_split"}
                ))
            
            logger.info(f"–£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ: —Å–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
            return chunks
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–º–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è: {e}")
            # Fallback –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ä–∞–∑–±–∏–µ–Ω–∏—é
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=overlap,
                separators=["\n\n", "\n", ". ", "! ", "? ", "; ", ", ", " ", ""]
            )
            return text_splitter.split_documents(pages)
    
    def _create_content_preview(self, pages: List, length: int = 20000) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            if not pages:
                return ""
            
            # –ë–µ—Ä–µ–º —Ç–µ–∫—Å—Ç —Å –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            content = ""
            for page in pages[:3]:  # –ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                content += page.page_content + "\n"
            
            if len(content) <= length:
                return content.strip()
            
            # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã
            preview = content[:length].strip()
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–∫–æ–Ω—á–∏—Ç—å –Ω–∞ –ø–æ–ª–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
            last_period = preview.rfind('.')
            if last_period > length * 0.7:
                preview = preview[:last_period + 1]
            
            return preview + "..."
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–µ–≤—å—é: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø—Ä–µ–≤—å—é"
    
    def _extract_metadata(self, file_path: str, pages: List) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        try:
            metadata = {
                'title': Path(file_path).stem,
                'pages': len(pages),
                'authors': '',
                'arxiv_id': ''
            }
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ ArXiv ID –≤ —Ç–µ–∫—Å—Ç–µ
            if pages:
                content = pages[0].page_content
                import re
                
                arxiv_patterns = [
                    r'arxiv:(\d+\.\d+)',
                    r'arXiv:(\d+\.\d+)',
                    r'(\d{4}\.\d{4,5})'
                ]
                
                for pattern in arxiv_patterns:
                    match = re.search(pattern, content, re.IGNORECASE)
                    if match:
                        metadata['arxiv_id'] = match.group(1)
                        break
            
            return metadata
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            return {
                'title': Path(file_path).stem,
                'pages': len(pages) if pages else 0,
                'authors': '',
                'arxiv_id': ''
            }
    
    def answer_question(self, question: str, conversation_history: List = None) -> Dict[str, Any]:
        """
        –û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å —á–µ—Ä–µ–∑ RAG —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –¥–∏–∞–ª–æ–≥–æ–≤ (–∫–∞–∫ –≤ notebook)
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            conversation_history: –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è conversational RAG
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            if not self.rag_chain:
                return {
                    'answer': "RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç.",
                    'source': 'error',
                    'quality': 'low'
                }
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º conversational RAG
            if conversation_history and len(conversation_history) > 1:
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º conversational RAG —Å Query Transformation")
                
                # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è conversational RAG
                messages = []
                for msg in conversation_history[-5:]:  # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π
                    if msg.get('role') == 'user':
                        messages.append(HumanMessage(content=msg.get('content', '')))
                    elif msg.get('role') == 'assistant':
                        messages.append(AIMessage(content=msg.get('content', '')))
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å
                messages.append(HumanMessage(content=question))
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º RAG —Ü–µ–ø–æ—á–∫—É —Å Query Transformation (–∫–∞–∫ –≤ notebook)
                answer = self.rag_query_transform_chain.invoke({"messages": messages})
                
                # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ —Ç–∏–ø–∞ "–î–∞", "–ù–µ—Ç" –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–ø—Ä–æ—Å –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
                if len(question.strip()) <= 3 and conversation_history:
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
                    last_user_question = None
                    for msg in reversed(conversation_history):
                        if msg.get('role') == 'user' and len(msg.get('content', '').strip()) > 3:
                            last_user_question = msg.get('content', '')
                            break
                    
                    if last_user_question:
                        logger.info(f"–ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç '{question}', –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–ø—Ä–æ—Å: '{last_user_question}'")
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
                        relevant_chunks = self.retriever.invoke(last_user_question)
                
            else:
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é RAG —Ü–µ–ø–æ—á–∫—É")
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é RAG —Ü–µ–ø–æ—á–∫—É (–∫–∞–∫ –≤ notebook)
                answer = self.rag_chain.invoke(question)
                
                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                relevant_chunks = self.retriever.invoke(question)
            
            # –ï—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –µ—â–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã (–¥–ª—è conversational RAG –±–µ–∑ –∫–æ—Ä–æ—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤)
            if 'relevant_chunks' not in locals():
                relevant_chunks = self.retriever.invoke(question)
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Ñ—Ä–∞–∑ "–Ω–µ –Ω–∞—à–µ–ª" –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            answer_cleaned = self._clean_answer(answer)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
            quality = self._analyze_answer_quality(question, answer_cleaned, relevant_chunks)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –æ—Ç–≤–µ—Ç–∞
            source = self._determine_answer_source(quality, relevant_chunks)
            
            logger.info(f"RAG –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question[:50]}... (–∫–∞—á–µ—Å—Ç–≤–æ: {quality}, –∏—Å—Ç–æ—á–Ω–∏–∫: {source}, —á–∞–Ω–∫–æ–≤: {len(relevant_chunks)})")
            logger.info(f"–û—Ç–≤–µ—Ç: {answer_cleaned[:100]}...")
            
            return {
                'answer': answer_cleaned,
                'source': source,
                'quality': quality,
                'chunks_used': len(relevant_chunks)
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞: {e}")
            return {
                'answer': f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}",
                'source': 'error',
                'quality': 'low'
            }
    
    def _clean_answer(self, answer: str) -> str:
        """
        –û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Ñ—Ä–∞–∑ "–Ω–µ –Ω–∞—à–µ–ª" –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        
        Args:
            answer: –ò—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç
            
        Returns:
            str: –û—á–∏—â–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        if not answer:
            return answer
        
        no_answer_phrases = ["–Ω–µ –Ω–∞—à–µ–ª –æ—Ç–≤–µ—Ç–∞", "—è –Ω–µ –Ω–∞—à–µ–ª"]
        answer_lower = answer.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ñ—Ä–∞–∑–∞ "–Ω–µ –Ω–∞—à–µ–ª"
        for phrase in no_answer_phrases:
            if phrase in answer_lower:
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –Ω–∞—á–∞–ª–∞ —Ñ—Ä–∞–∑—ã
                phrase_pos = answer_lower.find(phrase)
                # –ï—Å–ª–∏ –î–û —Ñ—Ä–∞–∑—ã –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (>30 —Å–∏–º–≤–æ–ª–æ–≤),
                # —É–±–∏—Ä–∞–µ–º —Ñ—Ä–∞–∑—É "–Ω–µ –Ω–∞—à–µ–ª" –∏ –≤—Å–µ —á—Ç–æ –ø–æ—Å–ª–µ –Ω–µ–µ
                if len(answer[:phrase_pos].strip()) > 30:
                    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –Ω–∞—á–∏–Ω–∞—è —Å "–Ω–µ –Ω–∞—à–µ–ª" –¥–æ –∫–æ–Ω—Ü–∞
                    cleaned = answer[:phrase_pos].strip()
                    logger.info(f"–û—á–∏—â–µ–Ω –æ—Ç–≤–µ—Ç: —É–¥–∞–ª–µ–Ω–æ '{phrase}' –∏ —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –Ω–µ–≥–æ (–±—ã–ª–æ {len(answer)} —Å–∏–º–≤–æ–ª–æ–≤, —Å—Ç–∞–ª–æ {len(cleaned)})")
                    return cleaned
        
        return answer
    
    def _analyze_answer_quality(self, question: str, answer: str, chunks: List) -> str:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""
        try:
            # –ë–æ–ª–µ–µ –≥–∏–±–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
            question_lower = question.lower()
            answer_lower = answer.lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
            question_words = set(question_lower.split())
            answer_words = set(answer_lower.split())
            
            logger.info(f"–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞: –≤–æ–ø—Ä–æ—Å='{question}', —Å–ª–æ–≤–∞ –≤–æ–ø—Ä–æ—Å–∞={question_words}")
            logger.info(f"–û—Ç–≤–µ—Ç: {answer[:200]}...")
            logger.info(f"–ß–∞–Ω–∫–∏ –Ω–∞–π–¥–µ–Ω—ã: {len(chunks)}")
            if chunks:
                logger.info(f"–ü–µ—Ä–≤—ã–π —á–∞–Ω–∫: {chunks[0].page_content[:200]}...")
                for i, chunk in enumerate(chunks):
                    logger.info(f"–ß–∞–Ω–∫ {i+1}: {chunk.page_content[:100]}...")
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
            common_words = question_words.intersection(answer_words)
            overlap_ratio = len(common_words) / len(question_words) if question_words else 0
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∏—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ –≤ –æ—Ç–≤–µ—Ç–µ
            key_words_in_answer = False
            for q_word in question_words:
                if len(q_word) > 3 and q_word in answer_lower:
                    key_words_in_answer = True
                    logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ '{q_word}' –≤ –æ—Ç–≤–µ—Ç–µ")
                    break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —á–∞–Ω–∫–∞—Ö
            chunks_content = " ".join([chunk.page_content.lower() for chunk in chunks])
            chunks_words = set(chunks_content.split())
            chunks_overlap = question_words.intersection(chunks_words)
            chunks_ratio = len(chunks_overlap) / len(question_words) if question_words else 0
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∏—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ (–¥–ª—è —Å–ª—É—á–∞–µ–≤ —Ç–∏–ø–∞ "–±–µ–≥–≥–∏–Ω–≥" vs "–±—ç–≥–≥–∏–Ω–≥")
            similar_words_found = False
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤ —Å —Ä–∞–∑–Ω—ã–º –Ω–∞–ø–∏—Å–∞–Ω–∏–µ–º
            word_variations = {
                '–±–µ–≥–≥–∏–Ω–≥': ['–±—ç–≥–≥–∏–Ω–≥', 'bagging'],
                '–±—ç–≥–≥–∏–Ω–≥': ['–±–µ–≥–≥–∏–Ω–≥', 'bagging'],
                'bagging': ['–±–µ–≥–≥–∏–Ω–≥', '–±—ç–≥–≥–∏–Ω–≥'],
                '–±—É—Å—Ç–∏–Ω–≥': ['–±—É—Å—Ç–∏–Ω–≥', 'boosting'],
                'boosting': ['–±—É—Å—Ç–∏–Ω–≥', '–±—É—Å—Ç–∏–Ω–≥'],
                '–∞–Ω—Å–∞–º–±–ª—å': ['ensemble'],
                'ensemble': ['–∞–Ω—Å–∞–º–±–ª—å']
            }
            
            for q_word in question_words:
                if len(q_word) > 3:  # –¢–æ–ª—å–∫–æ –¥–ª—è —Å–ª–æ–≤ –¥–ª–∏–Ω–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ —Å–ª–æ–≤–∞
                    variations_to_check = [q_word]
                    if q_word in word_variations:
                        variations_to_check.extend(word_variations[q_word])
                    
                    for chunk in chunks:
                        chunk_text = chunk.page_content.lower()
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ —Å–ª–æ–≤–∞
                        for variation in variations_to_check:
                            if variation in chunk_text:
                                similar_words_found = True
                                logger.info(f"–ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–µ–µ —Å–ª–æ–≤–æ: '{q_word}' -> '{variation}' –≤ —á–∞–Ω–∫–µ")
                                logger.info(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —á–∞–Ω–∫–∞: {chunk_text[:200]}...")
                                break
                        if similar_words_found:
                            break
                    if similar_words_found:
                        break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
            has_relevant_chunks = len(chunks) > 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º "–Ω–µ –Ω–∞—à–µ–ª"
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Ñ—Ä–∞–∑—É "–Ω–µ –Ω–∞—à–µ–ª" –≤ –æ—Ç–≤–µ—Ç–µ
            no_answer_phrases = ["–Ω–µ –Ω–∞—à–µ–ª –æ—Ç–≤–µ—Ç–∞", "—è –Ω–µ –Ω–∞—à–µ–ª"]
            has_no_answer_phrase = any(phrase in answer_lower for phrase in no_answer_phrases)
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ—Ä–∞–∑–∞ "–Ω–µ –Ω–∞—à–µ–ª", –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –î–û —ç—Ç–æ–π —Ñ—Ä–∞–∑—ã
            if has_no_answer_phrase:
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –Ω–∞—á–∞–ª–∞ —Ñ—Ä–∞–∑—ã "–Ω–µ –Ω–∞—à–µ–ª"
                no_answer_pos = min([
                    answer_lower.find(phrase) 
                    for phrase in no_answer_phrases 
                    if phrase in answer_lower
                ])
                # –ï—Å–ª–∏ –î–û —Ñ—Ä–∞–∑—ã "–Ω–µ –Ω–∞—à–µ–ª" –µ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–±–æ–ª—å—à–µ 30 —Å–∏–º–≤–æ–ª–æ–≤),
                # —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –æ—Ç–≤–µ—Ç –µ—Å—Ç—å, –ø—Ä–æ—Å—Ç–æ LLM –¥–æ–±–∞–≤–∏–ª –ª–∏—à–Ω–µ–µ –≤ –∫–æ–Ω—Ü–µ
                content_before_no = answer_lower[:no_answer_pos].strip()
                has_content_before = len(content_before_no) > 30
                is_standard_no_answer = not has_content_before and not key_words_in_answer
            else:
                is_standard_no_answer = False
            
            # –ë–æ–ª–µ–µ –≥–∏–±–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            logger.info(f"–ö—Ä–∏—Ç–µ—Ä–∏–∏: has_chunks={has_relevant_chunks}, is_no_answer={is_standard_no_answer}, key_words={key_words_in_answer}, similar={similar_words_found}")
            
            if has_relevant_chunks and not is_standard_no_answer:
                # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –æ—Ç–≤–µ—Ç–µ, –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ –∏–ª–∏ —Ö–æ—Ä–æ—à–µ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ - –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
                if key_words_in_answer or similar_words_found or overlap_ratio > 0.2 or chunks_ratio > 0.15:
                    logger.info(f"–í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: overlap={overlap_ratio:.2f}, chunks={chunks_ratio:.2f}, similar={similar_words_found}, key_words={key_words_in_answer}")
                    return 'high'
                elif overlap_ratio > 0.05 or chunks_ratio > 0.05:
                    logger.info(f"–°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: overlap={overlap_ratio:.2f}, chunks={chunks_ratio:.2f}")
                    return 'medium'
                else:
                    # –ï—Å–ª–∏ –µ—Å—Ç—å —á–∞–Ω–∫–∏, –Ω–æ –º–∞–ª–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π, –≤—Å–µ —Ä–∞–≤–Ω–æ —Å—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º
                    logger.info(f"–ß–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ: overlap={overlap_ratio:.2f}, chunks={chunks_ratio:.2f}, –Ω–æ –µ—Å—Ç—å —á–∞–Ω–∫–∏")
                    return 'medium'
            else:
                logger.info(f"–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: overlap={overlap_ratio:.2f}, chunks={chunks_ratio:.2f}, has_chunks={has_relevant_chunks}, is_no_answer={is_standard_no_answer}")
                return 'low'
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
            return 'low'
    
    def _determine_answer_source(self, quality: str, chunks: List) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –æ—Ç–≤–µ—Ç–∞"""
        if quality == 'high':
            return 'document'
        elif quality == 'medium':
            return 'document_partial'
        else:
            return 'not_found'
    
    def extract_document_topics(self) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ–º –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            if not self.vector_store:
                logger.warning("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
                return ["–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è —Å—Ç–∞—Ç—å–∏", "–ú–µ—Ç–æ–¥—ã –∏ –ø–æ–¥—Ö–æ–¥—ã", "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤—ã–≤–æ–¥—ã"]
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫
            # InMemoryVectorStore –Ω–µ –∏–º–µ–µ—Ç get_all_documents(), –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∏—Å–∫
            try:
                all_docs = self.vector_store.similarity_search("", k=1000)  # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {e}")
                return ["–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è —Å—Ç–∞—Ç—å–∏", "–ú–µ—Ç–æ–¥—ã –∏ –ø–æ–¥—Ö–æ–¥—ã", "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤—ã–≤–æ–¥—ã"]
            
            if not all_docs:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")
                return ["–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è —Å—Ç–∞—Ç—å–∏", "–ú–µ—Ç–æ–¥—ã –∏ –ø–æ–¥—Ö–æ–¥—ã", "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤—ã–≤–æ–¥—ã"]
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            full_text = " ".join([doc.page_content for doc in all_docs])
            
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            topics = self._extract_topics_from_text(full_text)
            
            logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(topics)} —Ç–µ–º –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            return topics
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–º: {e}")
            return ["–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è —Å—Ç–∞—Ç—å–∏", "–ú–µ—Ç–æ–¥—ã –∏ –ø–æ–¥—Ö–æ–¥—ã", "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤—ã–≤–æ–¥—ã"]
    
    def _extract_topics_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            import re
            
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
            topics = []
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–µ–º
            patterns = [
                r'## (.+)',  # Markdown –∑–∞–≥–æ–ª–æ–≤–∫–∏
                r'# (.+)',   # Markdown –∑–∞–≥–æ–ª–æ–≤–∫–∏
                r'Abstract[:\s]*(.+)',  # Abstract
                r'Introduction[:\s]*(.+)',  # Introduction
                r'Method[:\s]*(.+)',  # Method
                r'Result[:\s]*(.+)',  # Results
                r'Conclusion[:\s]*(.+)',  # Conclusion
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    topic = match.strip()[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                    if len(topic) > 10 and topic not in topics:
                        topics.append(topic)
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–µ–º, —Å–æ–∑–¥–∞–µ–º –æ–±—â–∏–µ
            if not topics:
                topics = [
                    "–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è —Å—Ç–∞—Ç—å–∏",
                    "–ú–µ—Ç–æ–¥—ã –∏ –ø–æ–¥—Ö–æ–¥—ã", 
                    "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤—ã–≤–æ–¥—ã"
                ]
            
            return topics[:3]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞–∫—Å–∏–º—É–º 3 —Ç–µ–º—ã
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞: {e}")
            return ["–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è —Å—Ç–∞—Ç—å–∏", "–ú–µ—Ç–æ–¥—ã –∏ –ø–æ–¥—Ö–æ–¥—ã", "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤—ã–≤–æ–¥—ã"]
    
    def _create_empty_vector_store(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        return InMemoryVectorStore(embedding=self.embeddings)
    
    def has_document(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        return self.vector_store is not None and self.rag_chain is not None
