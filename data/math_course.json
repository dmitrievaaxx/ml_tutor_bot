{
  "course": "Math",
  "lessons": [
    {
      "lesson_number": 1,
      "title": "Векторы и операции",
      "content": "Определение: Вектор — это упорядоченный набор чисел (компонентов), который можно представить как точку в n-мерном пространстве. Вектор записывается как v = [v₁, v₂, ..., vₙ], где каждый элемент vᵢ является координатой в соответствующем измерении.\n\nОсновные операции с векторами:\n\n1. Сложение векторов\na + b = [a1+b1, a2+b2, ..., an+bn]\nПример:\na = [3, 4], b = [1, 2]\na + b = [3+1, 4+2] = [4, 6]\n\n2. Умножение на скаляр\nk * a = [k*a1, k*a2, ..., k*an]\nПример:\nk = 2, a = [3, 4]\nk * a = [2*3, 2*4] = [6, 8]\n\n3. Скалярное произведение\na · b = a1*b1 + a2*b2 + ... + an*bn\nПример:\na = [3, 4], b = [1, 2]\na · b = 3*1 + 4*2 = 3 + 8 = 11\n\n4. Норма вектора\na = sqrt(a1^2 + a2^2 + ... + an^2)\nПример:\na = [3, 4]\na = sqrt(3^2 + 4^2) = sqrt(9 + 16) = sqrt(25) = 5\n\nПрименение в ML: Векторы используются для представления признаков объектов (feature vectors), хранения весов нейронных сетей и описания состояний систем. Скалярное произведение применяется для вычисления сходства между объектами."
    },
    {
      "lesson_number": 2,
      "title": "Матрицы и основные операции",
      "content": "Определение: Матрица — это прямоугольная таблица чисел, организованная в m строк и n столбцов. Матрица размера m×n записывается как A = [aᵢⱼ], где aᵢⱼ — элемент, расположенный в i-й строке и j-м столбце. Матрицы являются фундаментальными объектами линейной алгебры.\n\nОсновные операции с матрицами:\n1. Умножение матриц\nC = A × B, где c[i][j] = sum(A[i][k] * B[k][j])\nПример:\nДано: A = [[1, 2], [3, 4]], B = [[5, 6], [7, 8]]\nC[0][0] = 1*5 + 2*7 = 5 + 14 = 19\nC[0][1] = 1*6 + 2*8 = 6 + 16 = 22\nC[1][0] = 3*5 + 4*7 = 15 + 28 = 43\nC[1][1] = 3*6 + 4*8 = 18 + 32 = 50\nРезультат: C = [[19, 22], [43, 50]]\n\n2. Транспонирование\nA^T[i][j] = A[j][i]\nПример:\nДано: A = [[1, 2, 3], [4, 5, 6]]\nA^T = [[1, 4], [2, 5], [3, 6]]\n\n3. Детерминант (для квадратных матриц 2×2)\ndet(A) = a*d - b*c\nПример:\nДано: A = [[3, 1], [2, 4]]\ndet(A) = 3*4 - 1*2 = 12 - 2 = 10\n\nПрименение в ML: Матрицы используются для представления наборов данных (строки = объекты, столбцы = признаки), хранения весов нейронных сетей и выполнения линейных преобразований в алгоритмах машинного обучения."
    },
    {
      "lesson_number": 3,
      "title": "Собственные значения и векторы",
      "content": "Определение: Собственные значения и векторы — это фундаментальные понятия линейной алгебры.\nДля квадратной матрицы A собственный вектор v и собственное значение λ удовлетворяют уравнению\nAv = λv, где v ≠ 0.\n\nОсновные свойства:\n1) Вектор v называется собственным вектором матрицы A, если Av = λv для некоторого скаляра λ\n2) Скаляр λ называется собственным значением, если существует ненулевой вектор v такой, что Av = λv\n3) Характеристическое уравнение (в общем виде) для матрицы 2×2: det([[a - λ, b], [c, d - λ]]) = (a - λ)(d - λ) - bc = 0\n\nПример:\nПусть A = [[3, 1], [0, 2]]\n\nСоставим характеристическое уравнение:\ndet(A - λI) = (3 - λ)(2 - λ)\n\nРешаем (3 - λ)(2 - λ) = 0\n→ λ₁ = 3, λ₂ = 2\n\nДля λ = 3:\nA - 3I = [[0, 1], [0, -1]]\nРешаем (A - 3I)v = 0 → v₁ = (1, 0)\n\nДля λ = 2:\nA - 2I = [[1, 1], [0, 0]]\nРешаем (A - 2I)v = 0 → v₂ = (1, -1)\n\nОтвет:\nλ₁ = 3, v₁ = (1, 0)\nλ₂ = 2, v₂ = (1, -1)\n\nПрименение в ML:\nСобственные векторы используются в PCA для нахождения главных компонент данных, в анализе стабильности систем и сжатии информации. Собственные значения показывают, насколько «важно» каждое направление в данных."
    },
    {
      "lesson_number": 4,
      "title": "Ортогональность и проекции",
      "content": "Определение: Два вектора называются ортогональными, если их скалярное произведение равно нулю: a · b = 0.\n\nОсновные понятия:\n1. Ортогональные векторы: a · b = 0\n2. Ортонормированные векторы: ортогональные векторы единичной длины\n3. Проекция вектора a на вектор b: proj_b(a) = (a · b / |b|²) * b\n\nПримеры:\n- Векторы [1, 0] и [0, 1] ортогональны\n- Проекция [3, 4] на [1, 0] равна [3, 0]\n\nПрименение в ML: Ортогональность используется в PCA, QR-разложении и регуляризации нейронных сетей."
    },
    {
      "lesson_number": 5,
      "title": "SVD и PCA",
      "content": "SVD (Singular Value Decomposition): A = UΣV^T\n- U: левые сингулярные векторы\n- Σ: сингулярные значения\n- V: правые сингулярные векторы\n\nPCA (Principal Component Analysis): находит главные компоненты данных\n- Первая компонента: направление максимальной дисперсии\n- Последующие компоненты: ортогональны предыдущим\n\nПрименение в ML: Сжатие данных, снижение размерности, анализ главных компонент."
    },
    {
      "lesson_number": 6,
      "title": "Производные и частные производные",
      "content": "Определение: Производная функции f(x) в точке x₀ - это предел отношения приращения функции к приращению аргумента.\n\nЧастная производная: ∂f/∂x - производная по одной переменной при фиксированных остальных.\n\nОсновные правила:\n- (x^n)' = nx^(n-1)\n- (e^x)' = e^x\n- (ln x)' = 1/x\n- Произведение: (fg)' = f'g + fg'\n- Цепное правило: (f(g(x)))' = f'(g(x)) * g'(x)\n\nПрименение в ML: Градиентный спуск, оптимизация параметров нейронных сетей."
    },
    {
      "lesson_number": 7,
      "title": "Градиенты и цепное правило",
      "content": "Градиент: ∇f = [∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ]\n- Показывает направление наибольшего роста функции\n- Перпендикулярен линиям уровня\n\nЦепное правило: для композиции функций f(g(x))\n∂f/∂x = ∂f/∂g * ∂g/∂x\n\nПример:\nf(x) = (x² + 1)³\nf'(x) = 3(x² + 1)² * 2x = 6x(x² + 1)²\n\nПрименение в ML: Обратное распространение ошибки в нейронных сетях."
    },
    {
      "lesson_number": 8,
      "title": "Градиенты в матричной форме",
      "content": "Матричные производные:\n- ∂(Ax)/∂x = A^T\n- ∂(x^T A x)/∂x = (A + A^T)x\n- ∂(||Ax - b||²)/∂x = 2A^T(Ax - b)\n\nПримеры вычислений:\n- Градиент квадратичной формы\n- Градиент линейной функции\n- Градиент нормы\n\nПрименение в ML: Матричные вычисления в нейронных сетях, оптимизация."
    },
    {
      "lesson_number": 9,
      "title": "Градиентный спуск (GD, SGD)",
      "content": "Градиентный спуск: θ = θ - α∇J(θ)\n- α: скорость обучения\n- ∇J(θ): градиент функции потерь\n\nBatch GD: использует весь датасет\nSGD: использует один пример\nMini-batch GD: использует небольшую выборку\n\nПроблемы:\n- Локальные минимумы\n- Плато\n- Взрывающиеся градиенты\n\nПрименение в ML: Основной алгоритм обучения нейронных сетей."
    },
    {
      "lesson_number": 10,
      "title": "Adam и другие оптимизаторы",
      "content": "Adam (Adaptive Moment Estimation):\n- Адаптивная скорость обучения\n- Использует моменты первого и второго порядка\n- Комбинация RMSprop и Momentum\n\nДругие оптимизаторы:\n- Momentum: учитывает предыдущие градиенты\n- RMSprop: адаптивная скорость обучения\n- AdaGrad: накапливает квадраты градиентов\n\nСравнение:\n- Adam: быстрая сходимость, хорош для большинства задач\n- SGD: простой, но медленный\n- Momentum: помогает избежать локальных минимумов\n\nПрименение в ML: Современные оптимизаторы для глубокого обучения."
    },
    {
      "lesson_number": 11,
      "title": "Выпуклые и невыпуклые функции",
      "content": "Выпуклая функция: f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)\n- График лежит ниже хорды\n- Единственный глобальный минимум\n- Градиентный спуск гарантированно находит оптимум\n\nПримеры выпуклых функций:\n- Квадратичные функции\n- Экспоненциальные функции\n- Логарифмические функции\n\nНевыпуклые функции:\n- Множество локальных минимумов\n- Градиентный спуск может застрять\n- Пример: нейронные сети\n\nПрименение в ML: Понимание сложности оптимизации в глубоком обучении."
    },
    {
      "lesson_number": 12,
      "title": "Функции потерь (MSE, Cross-Entropy)",
      "content": "MSE (Mean Squared Error):\nL = (1/n) * Σ(y_pred - y_true)²\n- Для регрессии\n- Чувствительна к выбросам\n- Дифференцируема\n\nCross-Entropy:\nL = -Σ y_true * log(y_pred)\n- Для классификации\n- Логарифмическая шкала ошибок\n- Хорошо работает с вероятностями\n\nДругие функции потерь:\n- MAE (Mean Absolute Error)\n- Hinge Loss (SVM)\n- KL Divergence\n\nПрименение в ML: Выбор подходящей функции потерь для задачи."
    },
    {
      "lesson_number": 13,
      "title": "Регуляризация (L1, L2)",
      "content": "L2 регуляризация (Ridge):\nL = L_original + λ * Σ w²\n- Штрафует большие веса\n- Сохраняет все признаки\n- Градиент: 2λw\n\nL1 регуляризация (Lasso):\nL = L_original + λ * Σ |w|\n- Обнуляет неважные веса\n- Выполняет отбор признаков\n- Градиент: λ * sign(w)\n\nElastic Net: комбинация L1 и L2\n\nПрименение в ML: Предотвращение переобучения, отбор признаков."
    },
    {
      "lesson_number": 14,
      "title": "Случайные величины и распределения",
      "content": "Случайная величина: функция, сопоставляющая исходу опыта число.\n\nДискретные распределения:\n- Бернулли: P(X=1) = p, P(X=0) = 1-p\n- Биномиальное: P(X=k) = C(n,k) * p^k * (1-p)^(n-k)\n- Пуассона: P(X=k) = (λ^k * e^(-λ)) / k!\n\nНепрерывные распределения:\n- Нормальное: f(x) = (1/√(2πσ²)) * e^(-(x-μ)²/(2σ²))\n- Равномерное: f(x) = 1/(b-a) для x ∈ [a,b]\n- Экспоненциальное: f(x) = λe^(-λx)\n\nПрименение в ML: Моделирование данных, байесовские методы."
    },
    {
      "lesson_number": 15,
      "title": "Матожидание, дисперсия, ковариация",
      "content": "Математическое ожидание:\nE[X] = Σ x * P(X=x) (дискретное)\nE[X] = ∫ x * f(x) dx (непрерывное)\n\nДисперсия:\nVar(X) = E[(X - E[X])²] = E[X²] - (E[X])²\n\nКовариация:\nCov(X,Y) = E[(X - E[X])(Y - E[Y])]\n\nКорреляция:\nρ(X,Y) = Cov(X,Y) / (σ_X * σ_Y)\n\nСвойства:\n- E[aX + b] = aE[X] + b\n- Var(aX + b) = a²Var(X)\n- Cov(X,X) = Var(X)\n\nПрименение в ML: Анализ данных, нормализация признаков."
    },
    {
      "lesson_number": 16,
      "title": "Байесовская теорема",
      "content": "Теорема Байеса:\nP(A|B) = P(B|A) * P(A) / P(B)\n\nКомпоненты:\n- P(A|B): апостериорная вероятность\n- P(B|A): правдоподобие\n- P(A): априорная вероятность\n- P(B): нормализующая константа\n\nПример:\nP(болезнь|тест+) = P(тест+|болезнь) * P(болезнь) / P(тест+)\n\nНаивный Байес:\nP(класс|признаки) ∝ P(класс) * ∏ P(признак|класс)\n\nПрименение в ML: Классификация, обновление убеждений, байесовские сети."
    },
    {
      "lesson_number": 17,
      "title": "Maximum Likelihood Estimation (MLE)",
      "content": "MLE: находит параметры θ, максимизирующие правдоподобие данных.\n\nФункция правдоподобия:\nL(θ) = ∏ P(x_i|θ)\n\nЛогарифмическое правдоподобие:\nℓ(θ) = log L(θ) = Σ log P(x_i|θ)\n\nМаксимизация:\nθ* = argmax L(θ) = argmax ℓ(θ)\n\nПример: нормальное распределение\nμ_MLE = (1/n) * Σ x_i\nσ²_MLE = (1/n) * Σ (x_i - μ)²\n\nПрименение в ML: Обучение параметров моделей, оценка параметров распределений."
    },
    {
      "lesson_number": 18,
      "title": "Энтропия и дивергенции",
      "content": "Энтропия Шеннона:\nH(X) = -Σ P(x) * log P(x)\n- Мера неопределенности\n- Максимальна при равномерном распределении\n\nKL-дивергенция:\nD_KL(P||Q) = Σ P(x) * log(P(x)/Q(x))\n- Мера различия между распределениями\n- Несимметрична\n\nJS-дивергенция:\nD_JS(P||Q) = (1/2) * D_KL(P||M) + (1/2) * D_KL(Q||M)\nгде M = (P + Q) / 2\n\nПрименение в ML: Информационная теория, генеративные модели, метрики качества."
    }
  ]
}
