# Техническое видение проекта LLM-ассистент

## Технологии

Для создания простого и эффективного решения будем использовать следующий стек технологий:

* **Язык программирования:** `Python`
* **Telegram API:** `aiogram`
* **LLM API:** `OpenRouter` через реализацию OpenAI client 
* **Хранение данных:** Встроенные структуры Python(`dict`/`list`) для хранения истории диалогов
* **Тестирование:** `pytest`
* **Управление зависимостями:** `uv`
* **Автоматизация:** `Makefile`
* **Деплой:** `Docker`

## Принцип разработки

* **KISS** (Keep It Simple, Stupid) - максимальная простота решения 
* **Итеративный подход** - начинаем с MVP и постепенно добавляем функциональность
* **Функциональное программирование** - проектирование архитектуры через функции без ООП
* **Модульность** - разделение кода на логические компоненты
* **Минимализм** - только необходимый функционал
* **Чистый код** -  понятные имена, комментарии, документация
* **Быстрые итерации** - возможность быстро вносить изменения и тестировать 

## Структура проекта

```
/ 
├── bot/
│   ├── __init__.py                              
│   ├── main.py                 → запуск бота 
│   └── handlers.py             → обработка команд и сообщений
│
├── llm/
│   ├── __init__.py
│   └── client.py               → обёртка для работы с API LLM 
│
├── tests/
│   ├── __init__.py
│   └── test_bot.py             → базовые тесты логики 
│
├── doc/
│   ├── product_idea.md   
│   ├── vision.md   
│   ├── conventions.md
│   ├── tasklist.md 
│   └── workflow.md    
│
├── pyproject.toml 
├── Makefile 
├── Dockerfile 
├── .env 
└── README.md
```

## Архитектура проекта

Архитектура проекта следует принципам простоты и функциональности:

1. **Компонентный подход** – четкое разделение ответственности между модулями
2. **Функциональное программирование** – использование функций вместо классов
3. **Простая схема взаимодействия**: Telegram → Бот → LLM → Бот → Telegram


### Потоки данных

1. **Входящее сообщение**:
   - Пользователь отправляет сообщение в Telegram
   - `bot/main.py` получает событие и передает в соответствующий обработчик
   - `bot/handlers.py` обрабатывает сообщение и сохраняет в историю диалога

2. **Запрос к LLM**:
   - `bot/handlers.py` формирует контекст из истории диалога
   - `llm/client.py` создает запрос с системным промптом и историей
   - `llm/client.py` отправляет запрос к OpenRouter API

3. **Ответ пользователю**:
   - `llm/client.py` получает и обрабатывает ответ от LLM
   - `bot/handlers.py` сохраняет ответ в историю диалога
   - `bot/handlers.py` отправляет ответ пользователю через Telegram API

## Модель данных

Для хранения данных используем простые структуры данных Python^
1. **Хранение истории диалогов** в оперативной памяти с использованием структур данных Python (dict/list)
2. **Структура сообщения**:
```python
 {
    "role": "system" | "user" | "assistant",  # Роль отправителя сообщения
    "content": "текст сообщения"    # Содержимое сообщения
}
```
3. **Структура диалога**:
```python
{
    "chat_id": 123456789,    # ID чата в Telegram
    "messages": [    # История сообщений
        {"role": "system", "content": "Вы ассистент компании X, которая предоставляет услуги Y..."},
        {"role": "user", "content": "Привет"},
        {"role": "assistant", "content": "Здравствуйте! Чем могу помочь?"}
    ]
}
```


## Работа с LLM
Для взаимодействия с языковой моделью используем следующий подход:
* **API**: Используем OpenRouter API через реализацию OpenAI client
* **Системный промпт**: Включаем информацию о компании и услугах в системный промпт
* **Контекст**: Отправляем историю диалога для сохранения контекста общения
* **Обработка ответов**: Простая обработка ответов от LLM без дополнительной постобработки

```python
# Пример функции для работы с LLM
async def get_llm_response(dialog_history):
    response = await openai_client.chat.completions.create(
        model="openrouter/model_name",  # Конкретная модель через OpenRouter
        messages=dialog_history,
        temperature=0.7,
        max_tokens=90
    )
    return response.choices[0].message.content
```

## Монииторинг LLM
Для мониторинга работы LLM используем минималистичный подход:

**Логирование запросов и ответов LLM** - cохранение истории взаимодействий с LLM для анализа и отладки

## Сценарий работы 
Основные сценарии работы бота:

1. **Приветствие** — бот знакомится с пользователем и уточняет уровень знаний.
2. **Объяснение темы** — пользователь задаёт вопрос, бот объясняет понятие простыми словами с примерами и аналогиями.
3. **Уточнение** — пользователь задаёт дополнительные вопросы, бот расширяет или упрощает объяснение.
5. **Контекст** — бот запоминает ход диалога для связных ответов.



## Деплой 
Для развертывания бота используем простой и надежный подход:

1.  **Контейнеризация** – упаковка приложения в Docker-контейнер
2. **Локальный запуск** – запуск Docker-контейнера на локальной машине
3. **Простой процесс развертывания** – минимум шагов для запуска

## Подход к конфигурированию
Для конфигурации приложения используем простой и безопасный подход:

**Файл .env** - хранение всех конфигурационных параметров и переменных окружения:
* API ключи и токены
* Параметры LLM
* Системный промпт
* Другие настройки бота

## Подход к логированию
Для логирования используем стандартный и проверенный подход:

1. **Стандартный модуль Logging** – использование встроенного в Python модуля для логирования
2. **Логирование основных событий**:  

   * Запуск и остановка бота
   * Ошибки и исключения
   * Запросы и ответы LLM
3. **Уровни логирования**
   * `INFO` - стандартные информационные сообщения
   * `ERROR` – ошибки и исключения
   * `DEBUG` – подробная отладочная информация